# Natural-Language-Processing-in-TensorFlow
Natural Language Processing in TensorFlow by deeplearning.ai 
Link - ("https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome")


Natural Language Processing in TensorFlow
**********************************************************************************************************
**********************************************************************************************************
by deeplearning.ai ( https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome )
**********************************************************************************************************
**********************************************************************************************************

About this Course
536,928 recent views
If you are a software developer who wants to build scalable AI-powered algorithms, you need to understand how to use the tools to 
build them. This Specialization will teach you best practices for using TensorFlow, a popular open-source framework for machine 
learning.

In Course 3 of the deeplearning.ai TensorFlow Specialization, you will build natural language processing systems using TensorFlow.
You will learn to process text, including tokenizing and representing sentences as vectors, so that they can be input to
a neural network. You’ll also learn to apply RNNs, GRUs, and LSTMs in TensorFlow. Finally, you’ll get to train an  
LSTM on existing text to create original poetry!

The Machine Learning course and Deep Learning Specialization from Andrew Ng teach the most important and foundational principles
of Machine Learning and Deep Learning. This new deeplearning.ai TensorFlow Specialization teaches you how to use TensorFlow to 
implement those principles so that you can start building and applying scalable models to real-world problems. To develop a deeper 
understanding of how neural networks work, we recommend that you take the Deep Learning Specialization.

WHAT YOU WILL LEARN
**********************************************************************************************************
**********************************************************************************************************

Build natural language processing systems using TensorFlow

Process text, including tokenization and representing sentences as vectors

Apply RNNs, GRUs, and LSTMs in TensorFlow

Train LSTMs on existing text to create original poetry and more

**********************************************************************************************************
**********************************************************************************************************

Instructor
Instructor rating  -  4.67/5 (173 Ratings)
Laurence Moroney
AI Advocate
Google Brain
163,084 Learners
10 Courses


**********************************************************************************************************
**********************************************************************************************************

1.Sentiment in text
The first step in understanding sentiment in text, and in particular when training a neural network to do so is the tokenization
of that text. This is the process of converting the text into numeric values, with a number representing a word or a character. 
This week you'll learn about the Tokenizer and pad_sequences APIs in TensorFlow and how they can be used to prepare and encode text 
and sentences to get them ready for training neural networks!

**********************************************************************************************************
**********************************************************************************************************

2.Word Embeddings
Last week you saw how to use the Tokenizer to prepare your text to be used by a neural network by converting words into numeric
tokens, and sequencing sentences from these tokens. This week you'll learn about Embeddings, where these tokens are mapped as vectors
in a high dimension space. With Embeddings and labelled examples, these vectors can then be tuned so that words with similar
meaning will have a similar direction in the vector space. This will begin the process of training a neural network to udnerstand
sentiment in text -- and you'll begin by looking at movie reviews, training a neural network on texts that are labelled 'positive'
or 'negative' and determining which words in a sentence drive those meanings.

**********************************************************************************************************
**********************************************************************************************************

3.Sequence models
In the last couple of weeks you looked first at Tokenizing words to get numeric values from them, and then using Embeddings to
group words of similar meaning depending on how they were labelled. This gave you a good, but rough, sentiment analysis -- words 
such as 'fun' and 'entertaining' might show up in a positive movie review, and 'boring' and 'dull' might show up in a negative one.
But sentiment can also be determined by the sequence in which words appear. For example, you could have 'not fun', which of course is
the opposite of 'fun'. This week you'll start digging into a variety of model formats that are used in training models to understand
context in sequence!

**********************************************************************************************************
**********************************************************************************************************

4.Sequence models and literature
Taking everything that you've learned in training a neural network based on NLP, we thought it might be a bit of fun to turn the 
tables away from classification and use your knowledge for prediction. Given a body of words, you could conceivably predict the 
word most likely to follow a given word or phrase, and once you've done that, to do it again, and again. With that in mind, 
this week you'll build a poetry generator. It's trained with the lyrics from traditional Irish songs, and can be used to produce 
beautiful-sounding verse of it's own!

**********************************************************************************************************
**********************************************************************************************************

#THANKS for reading this.
